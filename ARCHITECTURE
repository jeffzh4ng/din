# Architecture
This is a high-level document which talks about hardware/software codesign: the
semantic gap. It explores various tradeoffs when optimizing for different
design points. For more specifics on parsing, code gen, or pipelined architectures,
take a look at [COMPILER](./COMPILER) or [CHIP](./CHIP).

**Contents**
1. Preface: What is computation?
2. ISA: semantic gap
3. Microarchitecture: latency vs throughput
4. References

### 1. Preface: What is computation
We solve high level problems by writing programs in which universal turing machines
interpret. In this timeline of the world, our physical machines are implemented
such that they orchestrate electrons to simulate nand calculus, rather than lambda.
However, because the two calculi can encode each other (isomorphism), they share
the same semantics.

Compilers and chips act in unison in order to bridge the semantic chasm that lies
between our high level thought and our low level electron cousins. The difference
between a compiler and a chip is that of function: a compiler translates while
a chip evaluates. Yes, a chip is really just an interpreter[0] implemented with gates.
If you write a software interpreter, you're simulating on a simulator. This is the
intuition why software interpreted languages are slower. Note that this axis is
orthogonal from source language features. You can have a language with static
types that is interpreted, and one with dynamic types that is compiled. With
engineering, the line always blurs — we even have JIT compilation.

--------------------- <-- SaaS: Web app, Mobile app
| Problem           |
--------------------- <-- ADT/AA: Sequences, Trees, Graphs
| Algorithm         |
--------------------- <-- HLL: C/C++/Java/Python        Runtime: Unix/Linux
| Compiler & OS     |
--------------------- <-- ISA: x86/ARM/RISC-V
| Microarchitecture |
--------------------- <-- RTL: Verilog/VHDL
| Gates             |
---------------------
| Electrons         |
---------------------

People often describe this system software and hardware as the soul of the machine,
which provides the bicycles our minds ride.

**Software vs Hardware**
The question of software vs hardware is to computation as nature vs nurture is
to humans (And in this context, by software I'm referring to system software,
not application software). One lens to interpret the problem is with
hardware/software codesign. There's a very strong flywheel-type argument that better
hardware enables better applications, which demand for better hardware. The
success stories of C/x86 enabling workloads like UNIX, and CUDA enabling ones such
as Alexnet/GPT, as well the failures of Itanium, suggest that you need both great
software and hardware.

<details>
<summary>Food for thought</summary>
Assuming Proebstings Law[^0], which is really being aware of:
1. compiler optimizations yield 4% speedup/year
2. rule of 72

then compiler-related optimizations yield a doubling in program speedup every
18 years. This pills one to think there's more room at the bottom[1] rather than
at the top.
</details>

**RISC vs CISC**
The flame war between the two camps ends up like any other debate over the
science of the artificial: ill-defined. For instance, at the HLL level, talking
about "paradigms" such as OOP vs FP is very ill-defined because many popular
languages pick and steal the best set of features from each other, such that you
can write code in either style within the same language.

Only when you analyze languages as *collections of features* will you truly be
able to reasoning about language design. Most HLL share a common set of features
such as being eager, sequential, and aliases as values, and differentiate with
other features.
- {risc/cisc} + cpus are both in this standard model
- cuda gpus are not sequential, but parallel

[0]: And actually, if you take a look inside the most RISCy CISC chips and the
     most CISCy RISC chips, you'll see a  __________. A chip's processor really
     implements a simpler language, whose interface we call Instruction Set
     Architectures (ISAs).

     It seems like the field still has science-envy, and wants to taxonify
     artificial creations. This is only useful for hackernews and reddit flame
     wars. In reality, the line blurs between OOP/FP, SQL/NoSQL, RISC/CISC,
     training/inference, when authors copy and steal the best set of features
     from each other.
[1]: see https://web.archive.org/web/20170105015142/http://www.its.caltech.edu/~feynman/plenty.html

### 1. ISA
**Semantic Gap**
Figure 1.
--------------------- <-- HLL: C/C++/Java/Python        Runtime: Unix/Linux
| Compiler & OS     |
--------------------- <-- ISA: x86/ARM/RISC-V
| Microarchitecture |
--------------------- <-- RTL: Verilog/VHDL

Recall figure 1, which is a simplified model of the computing stack. If you zoom
in, there's actually room to play around with the ISA's abstraction level: does
it get placed closer to the HLL or to the RTL? Take a look at figure 2.


Figure 2.
----------------------- HLL: C               ----------------------- HLL: C

----------------------- ISA: x86




                                             ----------------------- ISA: RISC-V


------------------------ RTL: Verilog        ----------------------- RTL: Verilog


We say the design on the left, where the ISA sits closer to the HLL has a small
semantic gap, whereas the design on the right, where the ISA sits closer to
the RTL has a large semantic gap. Clearly, the size of the semantic gap is from
the programmer's perspective.

Small semantic gap designs have simple compilers, and complex chips. Large semantic
designs have complex compilers, and simple chips. The division of labor concept
also affects not just the ISA, but also the microarchitecture.

- then RISC, John Cocke IBM 801, mid 70's (microcode??)
  - ignoring cultural reasons for now (teaching, and licensing)
  - https://www.ibm.com/history/john-cocke
  - https://news.ycombinator.com/item?id=33055361

**Instructions**
  - simple vs complex
  - fixed vs variable
  - unform decode vs non-uniform decode
  - few addressing modes vs lots of addresing modes

### 3. Microarchitecture: Latency vs Throughput
hybrid uarch today
  - hardwired vs microcoded. TODO: shao
    - wrong metal pictur of microcoded is some type of firmware. NO.
    - more useful
      - microcode are "exception handlers"
      - happens rarely

- Division of labor
- single core era: hardware took care of complexity
- multi core era: hardware pushed complexity to software
  --> shared state, message passing, data flow

**CPUs: latency-optimized machines**
- latency hiding
- latency reduction

- Latency-optimized will speculate on branch prediction and execute out-of-order (even if it throws away the work later... its better to take a chance to save a few nanoseconds at a time). 

- For the past fifty years, machines were designed for general-purpose computing,
largely enabled by smaller, and more efficient transistors.
- Semiconductor guys all got cracked up on Moore's Law.
- a "law" here doesn't refer to nature. it refers to a broader socioeconomic phenonemon
  that has self-fulfulling prophesies
- the truths about the world are simply the best lies
- economically, take a look at soros' theory of reflexivity.



                  20e9                                          * M1
                                                                * 
                                                                  
                                                              *  
                                                              *   
                                                            *    
                                                            *    
                                                            *     
                                                          **      
                                                        *        
                                                        **        
                                                      *          
                                                    **           
                                                    **            
                                                ***              
                                              ***  PDP-11              
                                            ***                   
                                        **** IBM 360                     
                                *******                          
                   1e4    ******* ENIAC                               

Bells Law: Calculators -> Mainframes -> Minis -> Workstations -> PC -> Mobile -> IoT

- moore's law enables bells law
- end?
  - amdahl's law

From the programmer's perspective, ISAs designed for general-purpose computing
specify two important semantic properties:
  1. stored program
  2. sequential instruction processing

We call these two high level properties the *von Neumann execution model*[^13].
It's what the end programmer implicitly assumes when programming in their favorite
high level language[^14]. To hammer the point home, realize that x86, the ISA,
is just an interface — they are indeed just specifications

**GPUs: throughput-optimized machines**
- Throughput-optimized computing doesn't want to waste any work, and instead has 10x SMT to have many, many threads ready to do work while waiting for branches / memory / etc. etc.

- actually very similar to von neumann architecture. might have heard of GPGPUS
- CUDA
- designed for pixels. not graphs.
- accelerators with graph/grid architecture we will see in nayru
- noob GPU tutorials:
  https://fgiesen.wordpress.com/2011/07/01/a-trip-through-the-graphics-pipeline-2011-part-1/
  https://litherum.blogspot.com/2023/10/implementing-gpus-programming-model-on.html?m=1
  https://learnopengl.com/
  https://vksegfault.github.io/posts/simd-usage-cpp-csharp-rust/
  numbat graphics stuff: https://github.com/TheNumbat/Lists

### 4. References
**Languages and Compilers**
- Cornell CS 4120 SP23 Lecture Notes (Myers)
- Engineering a Compiler (Cooper, Torczon)

**Computer Architecture**
- Computer Organization and Design RISC-V Edition: The Hardware Software Interface (Patterson, Hennessy)
- Digital Design and Computer Architecture, RISC-V Edition (Harris, Harris)